{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b442ace",
   "metadata": {},
   "source": [
    "# Building a Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "This tutorial demonstrates how to build a RAG system using LangChain and Google's embedding models. RAG combines retrieval of relevant documents with generative AI to provide contextually accurate responses.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Document loading and chunking strategies\n",
    "- Vector embeddings and similarity search\n",
    "- Building a retrieval system with ChromaDB\n",
    "- Querying knowledge bases effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc98038",
   "metadata": {},
   "source": [
    "## Step 1: Installing Dependencies\n",
    "\n",
    "We need several LangChain components for our RAG system:\n",
    "- **langchain-google-genai**: Google's embedding models for vector representations\n",
    "- **langchain-community**: Community tools including ChromaDB vector store\n",
    "- **chromadb**: In-memory vector database for storing and searching embeddings\n",
    "- **python-dotenv**: Environment variable management for API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264d7036-32a3-4fb5-8eab-363b94f07cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain langchain-community langchain-google-genai langchain-core langchain-text-splitters chromadb python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00265c3f",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "\n",
    "Setting up the core components for our RAG pipeline:\n",
    "- **Embeddings**: Convert text to numerical vectors for similarity search\n",
    "- **Vector Store**: Database to store and retrieve document embeddings\n",
    "- **Text Splitters**: Break large documents into manageable chunks\n",
    "- **Document Loaders**: Read and process text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a0c73-a5fa-4c7f-acbf-d5cf9e162676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import essential libraries for RAG system\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Google's embedding model\n",
    "from langchain_community.vectorstores import Chroma             # Vector database\n",
    "from langchain_core.documents import Document                   # Document structure\n",
    "from langchain_text_splitters import CharacterTextSplitter     # Text chunking\n",
    "from langchain_community.document_loaders import TextLoader    # File loading\n",
    "from dotenv import load_dotenv                                  # Environment variables\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6373635",
   "metadata": {},
   "source": [
    "## Step 3: Document Loading\n",
    "\n",
    "Loading our knowledge base from a text file. The TextLoader reads the entire file content and creates Document objects that contain both the text and metadata. This forms the foundation of our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb298f-230d-4d10-92eb-b4206839f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the laptop data from text file\n",
    "# TextLoader reads the entire file and creates Document objects\n",
    "loader = TextLoader(\"laptops_info.txt\")\n",
    "raw_docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8be0b",
   "metadata": {},
   "source": [
    "## Step 4: Text Chunking Strategy\n",
    "\n",
    "Breaking documents into smaller chunks improves retrieval accuracy. We use:\n",
    "- **chunk_size=500**: Each chunk contains ~500 characters (manageable for embeddings)\n",
    "- **chunk_overlap=50**: Overlapping text preserves context between chunks\n",
    "This ensures we don't lose important information at chunk boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc50d673-3d2b-4c35-a469-b5d411e8e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  5\n",
      "printing one of the document........\n",
      "page_content='3. ASUS TUF Gaming F15\n",
      "- Price: â‚¹79,990\n",
      "- CPU: Intel i5 11400H\n",
      "- GPU: NVIDIA RTX 3050 (4GB)\n",
      "- RAM: 16GB DDR4\n",
      "- Storage: 512GB SSD\n",
      "- Good for: Deep learning models, parallel processing\n",
      "- Comments: Rugged build, best performance for the price\n",
      "\n",
      "4. Acer Aspire 7\n",
      "- Price: â‚¹62,990\n",
      "- CPU: AMD Ryzen 5 5500U\n",
      "- GPU: NVIDIA GTX 1650\n",
      "- RAM: 8GB\n",
      "- Storage: 512GB SSD\n",
      "- Good for: Intro ML, data science\n",
      "- Comments: Value for money, limited by RAM/GPU' metadata={'source': 'laptops_info.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Split documents into chunks for better retrieval\n",
    "# Smaller chunks = more precise retrieval, larger chunks = more context\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "print(\"Total number of documents: \",len(docs))\n",
    "\n",
    "print(\"printing one of the document........\")\n",
    "print(docs[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed0e7a",
   "metadata": {},
   "source": [
    "## Step 5: Vector Embeddings Setup\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. We use Google's embedding-001 model which:\n",
    "- Creates 768-dimensional vectors\n",
    "- Captures semantic relationships between words\n",
    "- Enables similarity search based on meaning, not just keywords\n",
    "This allows us to find relevant documents even when query terms don't exactly match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7d4d0-0c11-415b-ae6f-119475536972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example embeddings........\n",
      "[0.05636945366859436, 0.004828543867915869, -0.07625909894704819, -0.023642510175704956, 0.053293220698833466]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Google's embedding model for vector representations\n",
    "# Get API key from: https://ai.google.dev/gemini-api/docs/api-key\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Create embedding model instance\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",  # Google's latest embedding model\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test embedding creation - converts text to 768-dimensional vector\n",
    "vector = embedding_model.embed_query(\"hello, world!\")\n",
    "print(\"example embeddings........\")\n",
    "print(vector[:5])  # Show first 5 dimensions\n",
    "len(vector)        # Full vector length: 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e198ea",
   "metadata": {},
   "source": [
    "## Step 6: Vector Store Creation\n",
    "\n",
    "ChromaDB creates a vector database from our document chunks. It:\n",
    "- Generates embeddings for each chunk using our embedding model\n",
    "- Stores both the vectors and original text\n",
    "- Builds indexes for fast similarity search\n",
    "This creates a searchable knowledge base where we can find relevant information quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f39e96-d98c-429e-b284-196ab9314705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from documents and embeddings\n",
    "# ChromaDB automatically generates embeddings for all document chunks\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2c948",
   "metadata": {},
   "source": [
    "## Step 7: Retriever Configuration\n",
    "\n",
    "The retriever is our search interface to the vector store. Configuration:\n",
    "- **search_type=\"similarity\"**: Finds documents most similar to the query\n",
    "- **k=2**: Returns top 2 most relevant chunks\n",
    "This balance ensures we get relevant context without overwhelming the system with too much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83dc9f1-66f8-477b-afa1-963cb95f05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure retriever for similarity search\n",
    "# k=2 means we'll get the 2 most relevant document chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b4b7d",
   "metadata": {},
   "source": [
    "## Step 8: Query Execution\n",
    "\n",
    "Testing our RAG system with a specific question. The retriever:\n",
    "1. Converts the query to an embedding vector\n",
    "2. Searches for similar vectors in the database\n",
    "3. Returns the most relevant document chunks\n",
    "This demonstrates the core RAG retrieval functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b5e07-a6a1-4fc9-ad47-ac133ee11524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test our RAG system with a specific query\n",
    "query = \"Which laptop is best for machine learning under ₹80,000?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(f\"Retrieved {len(retrieved_docs)} relevant documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f6afb",
   "metadata": {},
   "source": [
    "## Step 9: Results Analysis\n",
    "\n",
    "Examining the retrieved chunks helps us understand:\n",
    "- How well our chunking strategy worked\n",
    "- Whether relevant information was found\n",
    "- The quality of semantic matching\n",
    "These chunks would typically be fed to a language model to generate a comprehensive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d8028-4dff-47ab-ba28-4483bb0cee34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Retrieved Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "# Laptop Buying Tips for AI/ML (2024)\n",
      "- Prefer 16GB RAM or more\n",
      "- Look for NVIDIA GPUs like GTX 1650, RTX 3050 or better\n",
      "- Avoid integrated graphics for training models\n",
      "- SSD preferred for fast data access\n",
      "- Ryzen 5, i5 H-series or better recommended\n",
      "\n",
      "Chunk 2:\n",
      "# Laptop Buying Tips for AI/ML (2024)\n",
      "- Prefer 16GB RAM or more\n",
      "- Look for NVIDIA GPUs like GTX 1650, RTX 3050 or better\n",
      "- Avoid integrated graphics for training models\n",
      "- SSD preferred for fast data access\n",
      "- Ryzen 5, i5 H-series or better recommended\n"
     ]
    }
   ],
   "source": [
    "# Display the retrieved chunks to analyze retrieval quality\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\nChunk {i+1}:\\n{doc.page_content}\")\n",
    "    print(\"-\" * 50)  # Separator for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d08aac-c676-47ce-8df2-d8d69c3567d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
